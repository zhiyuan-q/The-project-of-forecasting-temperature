<h1 style="text-align: center; font-weight: bold;">气温预测项目</h1>

<h3 style="text-align: center; font-weight: bold;">The project of forecasting temperature</h3>

<h3 style="text-align: center; font-weight: bold;">邱志远</h3>

这是我利用随机森林实现的温度预测学习项目，其中的主要内容包括：

## **代码实现（小数据集）**

小数据集中只存有该地2024年一年的气温相关指标的数据，我的目的是利用随机森林实现基于该小数据集的气温相关指标进行温度预测的学习。

![](media/eb8440e4fb35ec353b2754786ed22808.png)

首先，利用使用pandas的read_csv函数读取CSV文件temps.csv，然后利用head函数和shape函数，打印出前五行的数据和temps.csv的数据维度，打印结果如下：

![](media/12c9cbcd4e809b9888cff0389618a91e.png)

说明该气温相关数据中共有9列，348行，每一列代表的含义分别是：

- ​    year：年
- ​    month：月
- ​    day：日
- ​    week：周几
- ​    temp_2：前天的最高气温
- ​    temp_1：昨天的最高气温
- ​    average：历史的今天最高气温的平均值
- ​    actual：当天实际的最高气温
- ​    other：其他数据

![](media/18ab8b290eaf3969e223168a167495c2.png)

其次，通过字符串拼接和strptime()函数，将离散的年、月、日转换为datetime对象，这样就可以将其转换为一个元组，当作坐标时可以被整体打印，方便后续基于时间的可视化，打印结果如下：

![](media/865e88216cb8d7df9e4ca65a5a1a2bb2.png)

接着利用代码绘制子图，绘制的四个子图如下：

![](media/e72b7a6d77d4f5f8c61ff77da0b2687e.png)

四个图分别显示了实际最高温度(actual)，昨天最高温度(temp_1)，前天最高温度(temp_2)，和其它观测温度(other)随时间的变化。

随后，使用pd.get_dummies对分类变量（如week列）进行独热编码，独热编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。例如week列进行独热编码后其效果如下：

![](media/3f8f640123ed95be31ed0bf56ab52ecd.png)

接下来，进行**数据集切分**，首先分离标签列(actual列)和特征，随后，使用train_test_split函数按75%-25%划分训练集和测试集，其中测试集占25%

紧接着构建随机森林回归模型预测温度，评估预测效果，使用1000棵决策树的随机森林模型，减少过拟合风险。随后计算平均绝对百分比误差（MAPE）对所构建模型的性能进行评估。

后面进行了决策树的可视化（导出第5棵决策树的图形结构）：

![](media/b64f83fed7445b24a77090574d9e0774.png)

如图，可以看出来不同子树的大小差异，但是如果直接绘制出决策树，节点太小不方便观察，所以我们只画一部分：

![](media/3564543e9792f830ae724238ab9cb6d0.png)

这样就可以清晰地看出节点的分裂规则以及每个节点所表示的信息。

在这之后，进行了特征重要性分析，计算各个特征的重要性，然后按重要性排序，结果输出如下：

![](media/8b6ccce11f00e47d4873ccbe98521924.png)

随后，打印用全部特征训练和用最重要的两个特征训练的mape值差异

![](media/5dd262da24d86324bcde1b8c11de22eb.png)

第一行的MAPE是用全部特征训练得到的，第二行的mape是用最重要的两个特征训练得到的，很明显，第一行的MAPE值更小，代表**平均绝对百分比误差更小**，拟合的效果更优，但是，用最重要的两个特征训练**花费的时间更少**，所以具体哪种方式更优还要看我们的数据量大小以及实际需求。

根据特征的重要性做出图表如下：

![](media/d696bf0241301143de094f5a329f57dd.png)

最后，再在同一个图像中，绘制真实值（蓝色曲线）和预测值（红色散点），画图效果如下：

![](media/4ea0289023f0afff4c36e596b8ed7ecf.png)

# **代码实现（大数据集）**

小数据集中存有该地2019-2025年六年的气温相关指标的数据，我的目的是利用随机森林实现基于该小大数据集的气温相关指标进行温度预测的学习。

![](media/7a7cbd698d1a741e1d0f9d7bedd8249d.png)

首先，利用使用pandas的read_csv函数读取CSV文件temps.csv，然后利用head函数和shape函数，打印出前五行的数据和temps.csv的数据维度，打印结果如下：

![](media/f0f5b9911bcaa4bc5f41f0ee12782136.png)

说明该气温相关数据中共有12列，2191行，每一列代表的含义分别是：

- ​    year：年
- ​    month：月
- ​    day：日
- ​    weekday：周几
- ​    ws_1：前一天的风速
- ​    prcp_1：前一天的降水
- ​    snwd_1：前一天的积雪深度
- ​    temp_2：前天的最高气温
- ​    temp_1：昨天的最高气温
- ​    average：历史的今天最高气温的平均值
- ​    actual：当天实际的最高气温
- ​    other：其他数据

![](media/18ab8b290eaf3969e223168a167495c2.png)

其次，通过字符串拼接和strptime()函数，将离散的年、月、日转换为datetime对象，这样就可以将其转换为一个元组，s当作坐标时可以被整体打印，方便后续基于时间的可视化，打印结果如下：

![](media/231b5aac16eac568f6bd2db6dbcb8ac4.png)

接着利用代码绘制子图，绘制的四个子图如下：

![](media/63195b02209ca7eabdfc09ac9d5949e8.png)

四个图分别显示了实际最高温度(actual)，昨天最高温度(temp_1)，前天最高温度(temp_2)，和其它观测温度(other)随时间的变化。

后面又绘制了四个子图，绘制的四个子图如下：

![](media/aa460a06778c01bb65e62114fb08618d.png)

从这四个子图中可以分析历史平均最高温度、风速、降水和积雪深度随时间的变化，进而探索气象因素（如风速、降水）对温度的可能影响。

![](media/79b8b90cb6ab2b7fb163d038be3978d8.png)

![](media/33180a8e1835b54604e9e5e242bf0b4e.png)然后进行了季节的创建（循环判断），根据月份划分季节，便于按季节分组分析温度和其他气象指标的关系，而且还要避免SettingWithCopyWarning，确保操作在副本上进行。

然后进行散点图的绘制，绘制的结果如下：

![](media/fbf879875b52988d9262ecaeb71f3113.png)

# **思考、分析与比较**

## **数据量多少对于结果的影响**

#### **小数据集：**

![](media/15dfe2a2e502dd911a925e1876f92307.png)

对大数据集（temps_extended.csv）读取并进行独热编码，处理分类变量，分离特征列和标签列（actual列），转换为NumPy数组。划分为训练集和测试集（75%训练，25%测试，随机种子0）。

![](media/d9e3e43541957cf6414c01a24b66de8f.png)

对小数据集（temps.csv）：同样进行独热编码，排除掉大数据集中有但是小数据集中没有的列（排除ws_1列、 prcp_1列、 snwd_1列）。划分训练集和测试集。

模型训练与测试：

![](media/133a39a1d49bcf93bfd8afd14ad2dd68.png)

使用小数据集的训练集训练随机森林回归模型（100棵树，随机种子0）。

测试数据来自大数据集的测试集，但仅选取与小数据集对应的特征（通过original_feature_indices）。

![](media/d1a3ce85093102ed14a6f4d6c4cfff79.png)

计算平均绝对误差（MAE）和基于MAPE的准确率（100 - MAPE），打印结果如下：

![](media/2df75328432c145b16b7058cbb872d96.png)
$$
\begin{flalign}
&注：\\
&评估指标公式\\

&1. 平均绝对误差 (MAE)\\

&MAE = \frac{1}{n} \sum_{i=1}^{n} \left| \text{预测值}_i - \text{真实值}_i \right|

\\
&2. 平均绝对百分比误差 (MAPE)\\

&MAPE = \left[ \frac{1}{n} \sum_{i=1}^{n} \frac{\left| \text{预测值}_i - \text{真实值}_i \right|}{\text{真实值}_i} \right] \times 100\%\\


&3. 准确率 (Accuracy)\\
&Accuracy = (1 - MAPE)\%

\end{flalign}
$$


#### **大数据集：**

![](media/747b6000210b4fcab87a10067c0c1ec4.png)

然后，使用大数据集的训练集（排除ws_1列、 prcp_1列、 snwd_1列）训练随机森林回归模型（100棵树，随机种子0）。

为了控制变量唯一，测试数据仍然来自大数据集的测试集，但仅选取与小数据集对应的特征（通过original_feature_indices）。

计算平均绝对误差（MAE）和基于MAPE的准确率（100 - MAPE），打印结果如下：

![](media/d56918661118c0585c74c042c27b59b7.png)

**可以看到，增大数据量可以使得机器学习后得到的模型误差更小。**

## **关于特征**

![](media/5a5f53bcd6b45937075cfc07bd15de0b.png)

如果在上一问的结尾的训练集中不排除大数据集中有但是小数据集中没有的特征（为了控制变量唯一，再次利用来自大数据集的测试集（但是测试集中的列是完整的）），打印的结果如下：

![](media/8a8f1368110cdad11a97de9a40610ed2.png)

**可见，加入新的特征也可以使得模型的效果提升。**

我们提出一个理论：
$$
\begin{flalign}
&如果将各个特征的重要性按照降序排序，设其重要性为φ_i (i=1,2,…,n)，若\\
&\exists k \in \mathbb{N}^*,\  \sum_{i=1}^{k} \varphi_i \geq 0.95\\
&\text{且若 } k > 1,\  \sum_{i=1}^{k-1} \varphi_i < 0.95\\
&那么利用前k个特征划分训练集和数据集，得出的模型在进行预测时的性价比比较高

\end{flalign}
$$
有了这个理论，我们接下来就来实践一下：

![](media/02cff352078f3da0e4d77f2726d5d08c.png)

首先，这段代码的作用是将特征的重要性倒序打印出来，打印出的结果如下图：

![](media/470f4b8f7b402513d2ec0dc62aeaa0f5.png)

有了特征的重要性，我们做一个特征及其重要性的图表，画图的代码如下：

![](media/c7502f4144c11e07b5065abb9002daed.png)

运行程序，这段代码画出的图像如下：

![](media/fae1fd3e0001bc3c46135e28749e0921.png)

从图像中我们就可以直观地看到不同特征的重要性之间的大小差异，从而对我们下一步对特征前k项的倒叙和的运用做了铺垫，下一步，我们尝试计算特征重要性按照倒叙排列后其前k项和，以及如何用图像更直观地显示出来其前k项和与95%的关系：

![](media/93153aad388596d1bac978d7f7ca5d01.png)

通过这段代码，可以对特征进行排序并对其重要性进行累加操作，下一步我们利用累加的结果进行画图：

![](media/15395be7a7b9110f081411f000d82abc.png)

利用这串代码，我们就可以很直观地看出特征重要性的前k项和与95%的关系，做出的折线图如下：

![](media/a28bd5710919640091922015cbd15639.png)

从图中我们可以看出从第五项开始，其特征的重要性之和就恰好开始大于等于95%。

当然，为了更加准确、严谨、客观，可以选择用代码来输出k的值，代码和输出的k值如下两图：

![](media/49a4f41aca87bae06e75fa68a2f03f10.png)

![](media/7d70d2fee1d6b090b213605bf1b3b558.png)

同样可以得出：从第五项开始，其特征的重要性之和就恰好开始大于等于95%。

也就是说，我们在下一步构建训练集和测试集的时候要选取的是重要性按降序排在前五的特征。

![](media/4322e7249de99b3d2a25bb7d4bdacbd6.png)

利用上述代码，就可以用按重要性降序排序的前五项特征创建训练集和测试集，训练后得到模型的评估结果如下：

![](media/58100c2d92644f09c7241012d78f5937.png)

因为特征数量的减少还有随机森林本身算法的因素，我们利用这种方式得到的模型确实误差增大、准确率下降，但注意一开始的理论中说得是“性价比”，性价比不仅仅跟最后**模型的准确度**有关，还跟从**开始学习到最后得到这个模型的时间**有关！

接下来，我尝试计算两个过程分别所需要的时间，而且我认为算一次算不准，所以我采用了计算十次取平均值的办法来确定最终所需要时间的数值，代码如下：

![](media/f49e065a098cfb7dee4ea41edb5185be.png)

求用前五个特征构造模型的时间同理，最后得到的结果如图所示：

![](media/5a2c5077b3ad66801facc86c6d5054f3.png)

可以看到，我们利用我们前面理论构建的模型，其准确率上相比直接构建略低一些，但耗时却少了很多，这对于数据集体量特别大的情况，无疑是很有“性价比”的一件事情，为了更方便地观察两种方法在准确率和运行时间方面的差异，我们将其变动的百分比打印了出来：

![](media/5a81b8f847220b6961c946497eb94675.png)

这进一步印证了一开始提出的理论，然后，我利用代码把基于小数据集特征构建的模型、基于大数据集特征构建的模型、基于我们的“性价比”理论构建的模型的误差、准确率和运行时间放在了图表中比较，打印出的图表如下：

![](media/73e3d2ab94c3e1f401697978ea38d737.png)

通过图表，我们可以更直观地感受到它们各项衡量指标的差别。

## **随机森林的参数选择**

由于在之前的代码中，我们直接令random_state = 42，这个当然不是最优解，但是我们如何寻找这个最优解呢，我们来尝试一下：

![](media/9c34c0c6aa06bd6712ad4960c8de9fa0.png)

在一开始，我们把特征列和标签列(actual列)分开，仍然选择用相同的方法来划分训练集和测试集。

![](media/09027e62f4f2231a5917557ec646bcd4.png)

我们选择六个比较重要的特征来构建模型，然后，我们打印这个模型的各个参数，看看现在的参数是怎样的：

![](media/d7189483d3fd7196dd01d42109392fd4.png)

### **随机搜索**

我们尝试利用随机搜索来寻找更好的参数来解决这个问题，我们选用的函数是：

RandomizedSearchCV()，这个函数可以帮助我们在候选集组合中，不断的随机选择一组合适的参数来建模，并且求其交叉验证后的评估结果。

![](media/47dfef48400c82f7d752f66327766d9d.png)

我们通过这种方式在候选集组合中，不断的随机选择一组合适的参数来建模，并且求其交叉验证后的评估结果，评估后的结果打印如下：

![](media/6f716a04624748251e73bac7c2581ad5.png)

我们尝试创建一个评估函数，对其优化后模型准确率的变化进行定量分析，其中添加epsilon = 1e-10 是为了避免真实值(test_lables)为零时导致除以零错误。

![](media/90e090ed143d6e8d75047f731e95f93f.png)

最后调用这个评估函数对我们的新老模型进行评估，评估的结果如下：

![](media/0656ccd5a32c32b04cfd4a105c1db21a.png)

可以看到，优化过后模型的误差变小了、准确率变大了，所以我们的优化方法起到了积极的作用。

### **网格搜索**

这次我们尝试利用随机搜索来寻找更好的参数来解决这个问题，我们选用的函数是：

GridSearchCV()，网格搜索，搜索的是参数，即在指定的参数范围内，按步长依次调整参数，利用调整的参数训练学习器，从所有的参数中找到在验证集上精度最高的参数，这其实是一个训练和比较的过程。

![](media/6a09b2cfc1c0542aa0a762d4ecdf2090.png)

首先，进行第一次网格搜索，利用参数网格再指定范围内枚举超参数组合，利用负平均绝对误差做评分指标，利用三折交叉验证，找到最优模型best_grid，利用evaluate进行评估，打印结果如下图所示：

![](media/e7e7ef52613f3c95ac6fc90bff7cad68.png)

![](media/4288377c5d55bbd0ffbc78500ea5dc91.png)

随后进行第二次网格搜索，进一步缩小超参数范围，对模型进行进一步优化，打印结果如下图所示：

![](media/d9371278075f69b5620196b320ef2013.png)

此时我们的Accuracy得到了进一步提升，我们打印此时的的参数值:

![](media/f3f86731c5d20d5fdb4c61e00320d388.png)

### **贝叶斯优化**

首先，定义了随机森林的交叉验证性能目标函数，同时以-MAPE值作为优化指标，随后设置了多维参数搜索空间（经过数据验证的有效范围）。

![](media/f25c17ba6f75ad15385899b8e5c1aa96.png)

后面，通过贝叶斯优化器进行了110次迭代搜索（10次初始随机搜索+100次定向优化）：

![](media/847c7326796bc05dd25cecf0978f8f3b.png)

在这之后，用得到的最佳参数训练最终的模型：

![](media/efc77b40225085098ba15932cc9af406.png)

最后，执行贝叶斯搜索并打印最后选用的参数组合和模型性能，打印结果如下：

![](media/950a861dc5a8f0b209af2a4a0f2bd584.png)

可见，使用贝叶斯优化对模型进行了进一步优化，使得Accuracy得到了进一步提升。

当然，做了这么多探究的工作，主要是为了探究三种方法对于模型的优化效果，我们最终追求的仍然是模型的泛化能力，即其在未来数据的预测中能否展现出优良的性能
